{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Predictive Tiering ML Model\n",
    "\n",
    "This notebook demonstrates the development of the machine learning model used to predict future object access. The goal is to build a model that can forecast the number of times an object will be accessed in the next 24 hours based on its current metadata.\n",
    "\n",
    "This prediction can then be used to **proactively migrate** objects to the appropriate storage tier *before* they become hot or cold, optimizing for cost and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we import the necessary libraries. We'll use `pandas` for data manipulation, `numpy` for numerical operations, and `scikit-learn` for building our regression model. We'll also use `matplotlib` and `seaborn` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Simulation\n",
    "\n",
    "In a real-world scenario, we would collect historical access data over time. For this hackathon, we'll simulate this data.\n",
    "\n",
    "We generate a dataset that mimics the metadata we track for each object, including:\n",
    "- `size_bytes`: The size of the object.\n",
    "- `access_count`: The total number of times the object has been accessed.\n",
    "- `creation_age_hours`: How long ago the object was created.\n",
    "- `last_accessed_age_hours`: How long ago the object was last accessed.\n",
    "- `version`: The version number of the object (increments on migration).\n",
    "\n",
    "Our target variable (`y`) is `next_24h_access_count`, which we'll also simulate based on a plausible relationship with the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_data(num_samples=1000):\n",
    "    print(f\"Generating {num_samples} samples of dummy data...\")\n",
    "    data = {\n",
    "        \"size_bytes\": np.random.randint(100, 1000000, num_samples),\n",
    "        \"access_count\": np.random.randint(0, 500, num_samples),\n",
    "        \"creation_age_hours\": np.random.uniform(0, 720, num_samples), # Up to 30 days old\n",
    "        \"last_accessed_age_hours\": np.random.uniform(0, 100, num_samples),\n",
    "        \"version\": np.random.randint(1, 5, num_samples),\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Simulate a plausible-looking target variable\n",
    "    # Heavily influenced by recent access, with some noise\n",
    "    df['next_24h_access_count'] = (\n",
    "        df['access_count'] / (df['last_accessed_age_hours'] + 1) * np.random.uniform(0.5, 1.5) + \n_    "        np.random.randint(0, 10, num_samples)\n",
    "    )\n",
    "    df['next_24h_access_count'] = df['next_24h_access_count'].astype(int).clip(lower=0)\n",
    "    \n",
    "    print(\"Dummy data generated.\")\n",
    "    return df\n",
    "\n",
    "df = generate_dummy_data(2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Let's quickly visualize the relationships in our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, x_vars=df.columns[:-1], y_vars=['next_24h_access_count'], height=4, aspect=0.8, kind='reg')\n",
    "plt.suptitle('Relationship of Features with Target Variable', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "We'll use a `RandomForestRegressor`, a powerful and interpretable ensemble model.\n",
    "\n",
    "1.  **Split Data**: We split our data into training and testing sets.\n",
    "2.  **Train Model**: We fit the model on the training data.\n",
    "3.  **Save Model**: The trained model is saved to a file (`model.joblib`) so our backend service can load and use it for live predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../backend/model.joblib\"\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "features = [\"size_bytes\", \"access_count\", \"creation_age_hours\", \"last_accessed_age_hours\", \"version\"]\n",
    "target = \"next_24h_access_count\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Now let's evaluate our model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An R-squared value close to 1.0 indicates that our model explains a large portion of the variance in the target variable, which is great.\n",
    "\n",
    "Let's also look at feature importances to see what the model considers most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `access_count` and `last_accessed_age_hours` are the most important predictors in our simulated data. This confirms the model is learning a sensible relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
